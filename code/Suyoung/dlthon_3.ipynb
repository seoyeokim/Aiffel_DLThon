{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting googletrans==4.0.0-rc1\n",
      "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: httpx==0.13.3 in /opt/conda/lib/python3.9/site-packages (from googletrans==4.0.0-rc1) (0.13.3)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.2.0)\n",
      "Requirement already satisfied: chardet==3.* in /opt/conda/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n",
      "Requirement already satisfied: httpcore==0.9.* in /opt/conda/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.1)\n",
      "Requirement already satisfied: hstspreload in /opt/conda/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2021.12.1)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in /opt/conda/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.5.0)\n",
      "Requirement already satisfied: idna==2.* in /opt/conda/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2021.10.8)\n",
      "Requirement already satisfied: h11<0.10,>=0.8 in /opt/conda/lib/python3.9/site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.0)\n",
      "Requirement already satisfied: h2==3.* in /opt/conda/lib/python3.9/site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.2.0)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in /opt/conda/lib/python3.9/site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (5.2.0)\n",
      "Requirement already satisfied: hpack<4,>=3.0 in /opt/conda/lib/python3.9/site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.0)\n",
      "Building wheels for collected packages: googletrans\n",
      "  Building wheel for googletrans (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17415 sha256=43d5b726b8118cf6d27f1c18ddb02dc151c8d2a52f5a9a00b616f40279dbf1c1\n",
      "  Stored in directory: /aiffel/.cache/pip/wheels/60/b3/27/d8aff3e2d5c2d0d97a117cdf0d5f13cd121e2c2b5fb49b55a0\n",
      "Successfully built googletrans\n",
      "Installing collected packages: googletrans\n",
      "  Attempting uninstall: googletrans\n",
      "    Found existing installation: googletrans 3.0.0\n",
      "    Uninstalling googletrans-3.0.0:\n",
      "      Successfully uninstalled googletrans-3.0.0\n",
      "Successfully installed googletrans-4.0.0rc1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install googletrans==4.0.0-rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import sacrebleu\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import googletrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85a03928f2284a8eaf2fdc6b989ef25d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab78d166eda4da8952d1155231623cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/649 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "178e670d933e4d1196882a9c73b51f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/529 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c9c96caa614f2fb6101b4ab6424f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MBart_tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65bfa13e51ba4562ae5d51ad3acebaa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.28G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MBart_model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = googletrans.Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_import(data_name):\n",
    "    data_path = os.getenv('HOME')+'/aiffel/project_data/dlthon/'+data_name\n",
    "    imported_data = pd.read_csv(data_path)\n",
    "    return imported_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigle_translate(input_data, s_lang, t_lang, top_k_val, type_trans = 'forward'):\n",
    "    original_list = []\n",
    "    return_list = []\n",
    "    class_list = []\n",
    "    MBart_tokenizer.src_lang = s_lang\n",
    "    if type_trans == 'forward':\n",
    "        single_data = input_data['conversation']\n",
    "        for i,row_input_data in enumerate(single_data):\n",
    "            retrun_text_list = []\n",
    "            text_sen = row_input_data\n",
    "            text_list = text_sen.split('\\n')\n",
    "            for text in text_list:\n",
    "                encoded_text = MBart_tokenizer(text, return_tensors=\"pt\")\n",
    "                generated_tokens = MBart_model.generate(**encoded_text, forced_bos_token_id = MBart_tokenizer.lang_code_to_id [t_lang])\n",
    "                translation = MBart_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "                retrun_text_list.extend(translation)\n",
    "            retrun_text = '\\n'.join(retrun_text_list)\n",
    "            class_list.append(input_data['class'][0])\n",
    "            original_list.append(text_sen)\n",
    "            return_list.append(retrun_text)\n",
    "            print(i)\n",
    "    else:\n",
    "        for i, text_sen in enumerate(input_data['conversation']):\n",
    "            retrun_text_list = []\n",
    "            text_list = text_sen.split('\\n')\n",
    "            for text in text_list:\n",
    "                encoded_text = MBart_tokenizer(text, return_tensors=\"pt\")\n",
    "                generated_tokens = MBart_model.generate(**encoded_text, forced_bos_token_id = MBart_tokenizer.lang_code_to_id [t_lang], do_sample=True, top_k=top_k_val)\n",
    "                translation = MBart_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "                retrun_text_list.extend(translation)\n",
    "            retrun_text = '\\n'.join(retrun_text_list)\n",
    "            class_list.append(input_data['class'][0])\n",
    "            original_list.append(text_sen)\n",
    "            return_list.append(retrun_text)\n",
    "            print(i)\n",
    "    \n",
    "    original_data = pd.DataFrame({'class' : class_list, 'conversation' : original_list})\n",
    "    translated_data = pd.DataFrame({'class' : class_list, 'conversation' : return_list})\n",
    "        \n",
    "    return original_data, translated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigle_translate_google(input_data, s_lang, t_lang, type_trans = 'forward'):\n",
    "    original_list = []\n",
    "    return_list = []\n",
    "    class_list = []\n",
    "    if type_trans == 'forward':\n",
    "        single_data = input_data['conversation']\n",
    "        for i, row_input_data in enumerate(single_data):\n",
    "            retrun_text_list = []\n",
    "            text_sen = row_input_data\n",
    "            text_list = text_sen.split('\\n')\n",
    "            for text in text_list:\n",
    "                translation = translator.translate(text, src=s_lang, dest=t_lang).text\n",
    "                retrun_text_list.append(translation)\n",
    "            retrun_text = '\\n'.join(retrun_text_list)\n",
    "            class_list.append(input_data['class'][0])\n",
    "            original_list.append(text_sen)\n",
    "            return_list.append(retrun_text)\n",
    "            print(i)\n",
    "    else:\n",
    "        for i, text_sen in enumerate(input_data['conversation']):\n",
    "            retrun_text_list = []\n",
    "            text_list = text_sen.split('\\n')\n",
    "            for text in text_list:\n",
    "                translation = translator.translate(text, src=s_lang, dest=t_lang).text\n",
    "                retrun_text_list.append(translation)\n",
    "            retrun_text = '\\n'.join(retrun_text_list)\n",
    "            class_list.append(input_data['class'][0])\n",
    "            original_list.append(text_sen)\n",
    "            return_list.append(retrun_text)\n",
    "            print(i)\n",
    "        \n",
    "    original_data = pd.DataFrame({'class' : class_list, 'conversation' : original_list})\n",
    "    translated_data = pd.DataFrame({'class' : class_list, 'conversation' : return_list})\n",
    "        \n",
    "    return original_data, translated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_translate(input_data, s_lang, t_lang, top_k_val):\n",
    "    original_data1, translated_data = sigle_translate(input_data, s_lang, t_lang, 'forward')\n",
    "    original_data2, return_data = sigle_translate(translated_data, t_lang, s_lang, top_k_val, 'backward')\n",
    "    return [original_data1, return_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_translate_google(input_data, s_lang, t_lang):\n",
    "    original_data1, translated_data = sigle_translate_google(input_data, s_lang, t_lang, 'forward')\n",
    "    original_data2, return_data = sigle_translate_google(translated_data, t_lang, s_lang, 'backward')\n",
    "    return [original_data1, return_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_translate_colabo(input_data, s_lang_g, t_lang_g, s_lang, t_lang, top_k_val):\n",
    "    original_data1, translated_data = sigle_translate_google(input_data, s_lang_g, t_lang_g, 'forward')\n",
    "    original_data2, return_data = sigle_translate(translated_data, t_lang, s_lang, top_k_val, 'backward')\n",
    "    return [original_data1, return_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bule(back_translate_data):\n",
    "    return_score = []\n",
    "    hypothesis = back_translate_data[1]['conversation']\n",
    "    references = back_translate_data[0]['conversation']\n",
    "    for hyp, ref in zip(hypothesis, references):\n",
    "        hypothese = [hyp]\n",
    "        reference = [ref]\n",
    "        bleu = sacrebleu.corpus_bleu(hypothese, [reference])\n",
    "        return_score.append(bleu.score)\n",
    "    mean_score = np.mean(return_score)\n",
    "    return mean_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_import('train.csv')\n",
    "\n",
    "train_data = train_data.drop(train_data.columns[0], axis=1)\n",
    "threat_data = train_data[train_data['class'] == '협박 대화']\n",
    "extort_data = train_data[train_data['class'] == '갈취 대화']\n",
    "co_bully_data = train_data[train_data['class'] == '직장 내 괴롭힘 대화']\n",
    "bully_data = train_data[train_data['class'] == '기타 괴롭힘 대화']\n",
    "\n",
    "threat_data.reset_index(drop=True, inplace=True)\n",
    "extort_data.reset_index(drop=True, inplace=True)\n",
    "co_bully_data.reset_index(drop=True, inplace=True)\n",
    "bully_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "threat_data = threat_data.astype(str)\n",
    "extort_data = extort_data.astype(str)\n",
    "co_bully_data = co_bully_data.astype(str)\n",
    "bully_data = bully_data.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "896"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(threat_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "threat_input_data = threat_data.sample(448).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threat_input_data_list = []\n",
    "k = 0\n",
    "\n",
    "for i in range(10, 451, 10):\n",
    "    threat_inputs = threat_input_data.loc[k:i]\n",
    "    threat_input_data_list.append(threat_inputs)\n",
    "    k+=10\n",
    "    \n",
    "len(threat_input_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>conversation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>협박 대화</td>\n",
       "      <td>너 너무 맘에 안들어 진짜 죽이고 싶어\\n같이 오늘 한번 아작을 내볼까?\\n미안해\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>협박 대화</td>\n",
       "      <td>안녕하세요\\n 네? 네 안녕하세요\\n 혹시 혈액형이 어떻게 되세요?\\n 네? 혈액형...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>협박 대화</td>\n",
       "      <td>내가 맘만 먹으면 너같은 거 여기 발도 못붙이게 할 수있는거 알지?\\n제가 뭘 잘못...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>협박 대화</td>\n",
       "      <td>너지?\\n뭐가?\\n내가 전직장에서 불미스러운 일에 엮여서 회사 여기로 옮긴거말이야\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>협박 대화</td>\n",
       "      <td>내 손에 뭐가 있는것같아?\\n에이 장난치지마세요\\n너 눈엔 이게 가짜 총으로 보이니...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>협박 대화</td>\n",
       "      <td>니네가족 한참 좋을때지?\\n죄송합니다. \\n죄송?이라는말 필요없고 내가 전에 얘기한...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>협박 대화</td>\n",
       "      <td>왜 그러세요 아저씨\\n여자 새끼들은 한결같이 얌체 같아 넌 살 자격이 없어\\n무슨 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>협박 대화</td>\n",
       "      <td>빨리 돈 구해와.\\n저 정말 돈 나올데가 없어요. 제발요.\\n니네 부모님 가게 정리...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>협박 대화</td>\n",
       "      <td>머해? 나 그것 좀 주라\\n응? 안돼. 이거 생인선물로 받은거야.\\n야 어차피 몰라...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>협박 대화</td>\n",
       "      <td>어제까지 완료 하기로 했던 일은 다 완료 되었나요?\\n어제 급한 일이 있어서 처리를...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>협박 대화</td>\n",
       "      <td>저기 아가씨 이쁘게 생겼네 \\n아 네. 감사합니다\\n내가 요 몇일 유심히 봤거든 \\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    class                                       conversation\n",
       "0   협박 대화  너 너무 맘에 안들어 진짜 죽이고 싶어\\n같이 오늘 한번 아작을 내볼까?\\n미안해\\...\n",
       "1   협박 대화  안녕하세요\\n 네? 네 안녕하세요\\n 혹시 혈액형이 어떻게 되세요?\\n 네? 혈액형...\n",
       "2   협박 대화  내가 맘만 먹으면 너같은 거 여기 발도 못붙이게 할 수있는거 알지?\\n제가 뭘 잘못...\n",
       "3   협박 대화  너지?\\n뭐가?\\n내가 전직장에서 불미스러운 일에 엮여서 회사 여기로 옮긴거말이야\\...\n",
       "4   협박 대화  내 손에 뭐가 있는것같아?\\n에이 장난치지마세요\\n너 눈엔 이게 가짜 총으로 보이니...\n",
       "5   협박 대화  니네가족 한참 좋을때지?\\n죄송합니다. \\n죄송?이라는말 필요없고 내가 전에 얘기한...\n",
       "6   협박 대화  왜 그러세요 아저씨\\n여자 새끼들은 한결같이 얌체 같아 넌 살 자격이 없어\\n무슨 ...\n",
       "7   협박 대화  빨리 돈 구해와.\\n저 정말 돈 나올데가 없어요. 제발요.\\n니네 부모님 가게 정리...\n",
       "8   협박 대화  머해? 나 그것 좀 주라\\n응? 안돼. 이거 생인선물로 받은거야.\\n야 어차피 몰라...\n",
       "9   협박 대화  어제까지 완료 하기로 했던 일은 다 완료 되었나요?\\n어제 급한 일이 있어서 처리를...\n",
       "10  협박 대화  저기 아가씨 이쁘게 생겼네 \\n아 네. 감사합니다\\n내가 요 몇일 유심히 봤거든 \\..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threat_input_data_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "threat_bt_colab0 = back_translate_colabo(threat_input_data_list[0], 'ko', 'en', 'ko_KR', 'en_XX', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "threat_bt_google0 = back_translate_google(threat_input_data_list[0], 'ko', 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "threat_bt0 = back_translate(threat_input_data_list[0], 'ko_KR', 'en_XX', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9631089451969896"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_bule(threat_bt_colab0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.8329663759924886"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_bule(threat_bt_google0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2771710719511318"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_bule(threat_bt0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
