{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_import(data_name):\n",
    "    data_path = os.getenv('HOME')+'/aiffel/project_data/dlthon/'+data_name\n",
    "    imported_data = pd.read_csv(data_path)\n",
    "    return imported_data\n",
    "\n",
    "def cleaning_sentence(sentence):\n",
    "        sentence = sentence.lower()\n",
    "        sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "        sentence = re.sub(r'\\([^)]*\\)', '', sentence)\n",
    "        sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "        sentence = re.sub(\"[^가-힣a-zA-Z0-9\\.\\?\\!,]+\", \" \", sentence)\n",
    "        sentence = re.sub(r'[\\n\\r]+', ' ', sentence)\n",
    "        sentence = sentence.strip()\n",
    "        return sentence\n",
    "\n",
    "def preprocess_sentence(data_list):\n",
    "    retrun_list = []\n",
    "    for sentence_frame in data_list:\n",
    "         befor_df = {}\n",
    "         conv_data = []\n",
    "         class_data = []\n",
    "         class_name = sentence_frame['class'][0]\n",
    "         for sentence in sentence_frame['conversation']:\n",
    "             cleaned_sentence = cleaning_sentence(sentence)\n",
    "             conv_data.append(cleaned_sentence)\n",
    "             class_data.append(class_name)\n",
    "         return_df = pd.DataFrame({'class' : class_data, 'conversation': conv_data})\n",
    "         retrun_list.append(return_df)\n",
    "    return retrun_list\n",
    "\n",
    "def random_deletion(text, prob=0.2):\n",
    "    words = text.split()\n",
    "    if len(words) == 1:\n",
    "        return text\n",
    "    return ' '.join([word for word in words if random.random() > prob])\n",
    "\n",
    "def random_swap(text, n=1):\n",
    "    words = text.split()\n",
    "    for _ in range(n):\n",
    "        idx1, idx2 = random.sample(range(len(words)), 2)\n",
    "        words[idx1], words[idx2] = words[idx2], words[idx1]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def convert_label(data):\n",
    "    label_map = {'협박 대화': 0, '갈취 대화': 1, '직장 내 괴롭힘 대화': 2, '기타 괴롭힘 대화': 3, '일반 대화': 4}\n",
    "    \n",
    "    data['class'] = data['class'].map(label_map)\n",
    "    return data\n",
    "\n",
    "def cal_len(train, test, rate):\n",
    "    con_data = np.concatenate((train, test), axis = 0)\n",
    "    seg_len = []\n",
    "    spl_len = []\n",
    "    for i in con_data:\n",
    "        single_seg_len = len(i)\n",
    "        seg_len.append(single_seg_len)\n",
    "    for i in con_data:\n",
    "        single_spl_len = len(i.split())\n",
    "        spl_len.append(single_spl_len)\n",
    "    spl_len.sort()\n",
    "    seg_len.sort()\n",
    "    print('spl len is : ', spl_len[int(len(spl_len)*rate)])\n",
    "    print('seg len is : ', seg_len[int(len(seg_len)*rate)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_aug(data_list, prob, n):\n",
    "    len_data = []\n",
    "\n",
    "    for data in data_list:\n",
    "        len_data.append(len(data))\n",
    "\n",
    "    max_len_of = max(len_data)\n",
    "\n",
    "    return_data = []\n",
    "\n",
    "    for data_set in data_list:\n",
    "        if len(data_set) != max_len_of:\n",
    "            conver_data = []\n",
    "            class_Data = []\n",
    "            return_df = {}\n",
    "            aug_len = max_len_of - len(data_set)\n",
    "            class_name = data_set['class'][0]\n",
    "            for i in range(aug_len): class_Data.append(class_name)\n",
    "            for i in range(aug_len):\n",
    "                choice_num = random.random()\n",
    "                random_seq = data_set['conversation'].sample(1).iloc[0]\n",
    "                if choice_num >= 0.5:\n",
    "                    output_seq = random_deletion(random_seq, prob)\n",
    "                    conver_data.append(output_seq)\n",
    "                else:\n",
    "                    output_seq = random_swap(random_seq, n)\n",
    "                    conver_data.append(output_seq)\n",
    "                    \n",
    "            retrun_df = pd.DataFrame({'class':class_Data, 'conversation':conver_data})\n",
    "        else:\n",
    "            retrun_df = 0\n",
    "\n",
    "        return_data.append(retrun_df)\n",
    "\n",
    "    final_list = []\n",
    "\n",
    "    for auged_data, real_data in zip(return_data, data_list):\n",
    "        if isinstance(auged_data, pd.DataFrame):\n",
    "            real_data = pd.concat([real_data, auged_data])\n",
    "            real_data.reset_index(drop=True, inplace=True)\n",
    "            final_list.append(real_data)\n",
    "        else:\n",
    "            final_list.append(real_data)\n",
    "    \n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertModel.\n",
      "\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "bert_model = TFAutoModel.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2', output_hidden_states=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_encode(datas, sent_max_length):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for sent in datas:\n",
    "        encoded = tokenizer.encode_plus(sent,\n",
    "                                        add_special_tokens = True,\n",
    "                                        max_length = sent_max_length,\n",
    "                                        padding='max_length',\n",
    "                                        truncation = True,\n",
    "                                        return_attention_mask=True)\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    return np.array(input_ids), np.array(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(bert_model, max_len):\n",
    "    input_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32)\n",
    "    attention_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32)\n",
    "    \n",
    "    output = bert_model([input_ids, attention_mask])\n",
    "    output = output.last_hidden_state\n",
    "    output = tf.keras.layers.Dense(64, activation='relu')(output)\n",
    "    output = tf.keras.layers.Dropout(0.2)(output)\n",
    "    output = tf.keras.layers.Dense(3, activation='softmax')(output)\n",
    "    \n",
    "    model = tf.keras.Model(inputs = [input_ids, attention_mask], outputs = output)\n",
    "    model.compile(Adam(learning_rate=0.0001),loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_import('train_arg.csv')\n",
    "train_data = train_data.drop(['data_type', 'idx'], axis=1)\n",
    "nomal_data = data_import('preprocessed_train_raw.csv')\n",
    "nomal_data = nomal_data[nomal_data['class'] == 4]\n",
    "nomal_data['class'] = nomal_data['class'].map({4 : '일반 대화'})\n",
    "nomal_data = nomal_data.sample(20000)\n",
    "nomal_data = nomal_data.drop('Unnamed: 0', axis=1)\n",
    "nomal_data = nomal_data[['class', 'conversation']]\n",
    "all_data = pd.concat([train_data, nomal_data])\n",
    "all_data = all_data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "proto_data = np.array(all_data['conversation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "changed_data = []\n",
    "\n",
    "for sentence in proto_data:\n",
    "    sentence = re.sub(r'\\n', ' 옌 ', sentence)\n",
    "    changed_data.append(sentence)\n",
    "\n",
    "prot_data = np.array(changed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['믿었던 사람이 나를 버렸어. 아무도 만나고 싶지 않아. 옌 믿음이 컸던 사람이 떠나서 더 마음이 아프시겠고 사람을 쉽게 믿지 못하시겠어요. 옌 믿음이 컸었는데 한순간에 떠나버렸어. 앞으로는 그 누구와도 사귀지 않을 예정이야. 옌 어떻게 하면 지금의 감정을 완화시킬 수 있을까요? 옌 당분간 혼자 있으면서 생각도 정리하고 마음의 안식을 갖는 시간을 가져 볼까 해. 옌 혼자만의 시간이 의미 있는 시간으로 받아들여졌으면 좋겠어요.',\n",
       "       '치매에 걸려 딸을 잘 알아보지 못하는 나 자신이 너무 싫어. 옌 몸이 안 좋으신 일로 자기비난을 하시는 거군요. 옌 길을 찾지 못해서 밖을 나가지도 못해. 나 내 정신 탓이야. 옌 자기 비난에 빠진 감정을 어떻게하면 바꿀 수 있을까요? 옌 병을 최대한 치료하고 늦출 수 있게 노력해야겠지. 옌 몸조리를 잘해서 최대한 진행을 늦출 수 있었으면 좋겠어요.',\n",
       "       '움직이기가 너무 귀찮아 운동을 좀 해야 하는데. 옌 요즘 하시는 운동이 있으세요? 옌 걷기라도 해야 하는데 꼼짝하기가 싫어. 옌 운동에 대한 생각은 있으신데 성가심을 느끼고 계시군요.  옌 맞아. 그렇지만 오늘은 저녁 먹고 슬슬 나가봐야겠어. 옌 막상 나가시면 뿌듯하실 거예요.',\n",
       "       ...,\n",
       "       '저기요. 쿠팡잇츠에서 시킨 사람인데요. 빙수 다른데요? 옌 네? 혹시 주문번호가 어떻게 되나요.? 옌 203번이요 옌 아. 사인머스켓 시켜셨지요? 문제인가요? 옌 네. 샤인머스켓에서 단맛이 없고 사과처럼 퍼석 옌 그럴리가 다른 고객님들은 아직 컴플레인 혹시 불 옆에서 하시거나 다른 음식이랑 아니신가요? 옌 팥빙수를 그냥 먹지 조리를 어떻게 미친거아니야. 그리고 내가 다른 음식이랑 왜 섞어서 먹어요. 됐고 빨리 환불해주세요. 옌 분 음식이 어떻게 할까요? 옌 또 우리만 손해니까 그냥 환불해드려. 옌 네. 알겠습니다. 고객님 들여 지금 환불해드리겠습니다.',\n",
       "       '아이들을 가르치는 일이 좋아서 시작했는데 이젠 교탁에 서 있기도 힘에 부쳐. 이젠 무리야. 옌 몸이 아파서 정리를 고민하시다니 아주 속상하시겠어요. 옌 배로 낳은 아이와 같이 사랑으로 보듬었는데 그 체력이 다한 것 같아 눈물이 나네. 옌 지금 상황에서 할 수 있는 최선의 방법이 무엇일까요? 옌 최대한 오래 일을 하고 싶지만 교장 선생님과 면담을 해서 퇴직 시기를 정해야겠어. 옌 면담을 통해 원활한 조율을 이루시길 바라요.',\n",
       "       '내 생일인데 축하 한마디 없네. 아들 키워봤자 다 소용없어. 옌 축하를 못 받아 기분이 안 좋으시군요. 옌 돈 쓰고 놀러 다닐 줄만 알지. 부모님 생각은 요만큼도 안 해. 옌 이 상황에서 어떻게 하실지 생각해 보셨나요? 옌 남편이랑 같이 맛있는 거나 먹고 와야겠어. 옌 남편분과 같이 좋은 시간 가졌으면 좋겠어요.'],\n",
       "      dtype='<U896')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_a = np.array(prot_data[:int(0.8*len(prot_data))], dtype = str)\n",
    "test_data_a = np.array(prot_data[int(0.8*len(prot_data)):], dtype = str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spl len is :  240\n",
      "seg len is :  896\n"
     ]
    }
   ],
   "source": [
    "cal_len(train_data_a, test_data_a, 0.999999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ids_a, train_attention_mask_a = bert_encode(train_data_a, 240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 6, 246458, 2]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode('옌'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     0,  34542,  16595,    480,  29072,   9178, 118100,    413,\n",
       "        11817, 153484,   1190, 167003,   8000, 136214,  31369, 233604,\n",
       "            5,      6, 246458,  43972,    469,   6888,      6,  87648,\n",
       "        28644,   5301,   6571,   1083,  57049,   4274,  22738,    413,\n",
       "            6, 106343,  11470, 160930,      5,      6, 246458, 175926,\n",
       "            6,  77925,   1190,  22058,  23069,      6, 164540,    413,\n",
       "       121977,  57953,  22058,   1963,      5,   8000,   5920,  97053,\n",
       "            6, 239357,    469,   6838,      5,      6, 246458,  57049,\n",
       "         7091,  22738,    480,  89392,   4033, 143987,    413,  35078,\n",
       "        17801,  11095, 170801,   1020, 154325,   5144,     32,      6,\n",
       "       246458,  80332,    413, 207145,  70439,   2382,      6, 249950,\n",
       "        14842,   1020,  92234,  54122,  19353,  54232,   1190,      5,\n",
       "            6, 246458,  43972,   6740,  80860,  11817,  23069, 207145,\n",
       "        19217,    413,      6, 249950,  14842,   1020,  72121,  37557,\n",
       "       132198,  38736,      5,      2,      1,      1,      1,      1,\n",
       "            1,      1,      1,      1,      1,      1,      1,      1,\n",
       "            1,      1,      1,      1,      1,      1,      1,      1,\n",
       "            1,      1,      1,      1,      1,      1,      1,      1,\n",
       "            1,      1,      1,      1,      1,      1,      1,      1,\n",
       "            1,      1,      1,      1,      1,      1,      1,      1,\n",
       "            1,      1,      1,      1,      1,      1,      1,      1,\n",
       "            1,      1,      1,      1,      1,      1,      1,      1,\n",
       "            1,      1,      1,      1,      1,      1,      1,      1,\n",
       "            1,      1,      1,      1,      1,      1,      1,      1,\n",
       "            1,      1,      1,      1,      1,      1,      1,      1,\n",
       "            1,      1,      1,      1,      1,      1,      1,      1,\n",
       "            1,      1,      1,      1,      1,      1,      1,      1,\n",
       "            1,      1,      1,      1,      1,      1,      1,      1,\n",
       "            1,      1,      1,      1,      1,      1,      1,      1,\n",
       "            1,      1,      1,      1,      1,      1,      1,      1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_ids_a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_laber(data, index_num, max_len):\n",
    "    return_data = []\n",
    "    \n",
    "    for conv in data:\n",
    "        config_num = 0\n",
    "        return_sigle_sent = []\n",
    "        for sen in conv:\n",
    "            if sen == index_num:\n",
    "                if config_num == 0:\n",
    "                    config_num = 1\n",
    "                else:\n",
    "                    config_num = 0\n",
    "            elif sen == 1:\n",
    "                return_sigle_sent.append(2)\n",
    "            else :\n",
    "                return_sigle_sent.append(config_num)\n",
    "        return_sigle_sent = np.array(return_sigle_sent)\n",
    "        if return_sigle_sent.shape[0] != max_len:\n",
    "            add_len = max_len - return_sigle_sent.shape[0]\n",
    "            num_in = return_sigle_sent[-1]\n",
    "            for i in range(add_len):\n",
    "                return_sigle_sent = np.append(return_sigle_sent, int(num_in))\n",
    "            return_sigle_sent = np.reshape(return_sigle_sent, (1, max_len))\n",
    "        return_data.append(return_sigle_sent)\n",
    "    \n",
    "    return return_data        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_ids = make_laber(train_input_ids_a,246458, 240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_ids_np = np.concatenate(train_label_ids, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_ids_np[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "changed_data = []\n",
    "\n",
    "for sentence in proto_data:\n",
    "    sentence = re.sub(r'[\\n\\r]+', ' ', sentence)\n",
    "    changed_data.append(sentence)\n",
    "\n",
    "proto_data = np.array(changed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_b = np.array(proto_data[:int(0.8*len(proto_data))], dtype = str)\n",
    "test_data_b = np.array(proto_data[int(0.8*len(proto_data)):], dtype = str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a_input_ids_b, train_a_attention_mask_b = bert_encode(train_data_b, 240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     0,  34542,  16595,    480,  29072,   9178, 118100,    413,\n",
       "        11817, 153484,   1190, 167003,   8000, 136214,  31369, 233604,\n",
       "            5,  43972,    469,   6888,      6,  87648,  28644,   5301,\n",
       "         6571,   1083,  57049,   4274,  22738,    413,      6, 106343,\n",
       "        11470, 160930,      5, 175926,      6,  77925,   1190,  22058,\n",
       "        23069,      6, 164540,    413, 121977,  57953,  22058,   1963,\n",
       "            5,   8000,   5920,  97053,      6, 239357,    469,   6838,\n",
       "            5,  57049,   7091,  22738,    480,  89392,   4033, 143987,\n",
       "          413,  35078,  17801,  11095, 170801,   1020, 154325,   5144,\n",
       "           32,  80332,    413, 207145,  70439,   2382,      6, 249950,\n",
       "        14842,   1020,  92234,  54122,  19353,  54232,   1190,      5,\n",
       "        43972,   6740,  80860,  11817,  23069, 207145,  19217,    413,\n",
       "            6, 249950,  14842,   1020,  72121,  37557, 132198,  38736,\n",
       "            5,      2,      1,      1,      1,      1,      1,      1,\n",
       "            1,      1,      1,      1,      1,      1,      1,      1,\n",
       "            1,      1,      1,      1,      1,      1,      1,      1,\n",
       "            1,      1,      1,      1,      1,      1,      1,      1,\n",
       "            1,      1,      1,      1,      1,      1,      1,      1,\n",
       "            1,      1,      1,      1,      1,      1,      1,      1,\n",
       "            1,      1,      1,      1,      1,      1,      1,      1,\n",
       "            1,      1,      1,      1,      1,      1,      1,      1,\n",
       "            1,      1,      1,      1,      1,      1,      1,      1,\n",
       "            1,      1,      1,      1,      1,      1,      1,      1,\n",
       "            1,      1,      1,      1,      1,      1,      1,      1,\n",
       "            1,      1,      1,      1,      1,      1,      1,      1,\n",
       "            1,      1,      1,      1,      1,      1,      1,      1,\n",
       "            1,      1,      1,      1,      1,      1,      1,      1,\n",
       "            1,      1,      1,      1,      1,      1,      1,      1,\n",
       "            1,      1,      1,      1,      1,      1,      1,      1,\n",
       "            1,      1,      1,      1,      1,      1,      1,      1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_a_input_ids_b[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_a_attention_mask_b[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 240)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 240)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     TFBaseModelOutputWit 117653760   input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 240, 64)      24640       tf_bert_model[0][13]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 240, 64)      0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 240, 3)       195         dropout_37[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 117,678,595\n",
      "Trainable params: 117,678,595\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "use_bert_model = create_model(bert_model, 240)\n",
    "use_bert_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\"dlthon2.keras\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max', save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "2547/2547 [==============================] - 642s 246ms/step - loss: 0.1692 - accuracy: 0.9091 - val_loss: 0.1279 - val_accuracy: 0.9328\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.93280, saving model to dlthon2.keras\n",
      "Epoch 2/10\n",
      "2547/2547 [==============================] - 625s 246ms/step - loss: 0.1196 - accuracy: 0.9399 - val_loss: 0.1197 - val_accuracy: 0.9400\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.93280 to 0.94005, saving model to dlthon2.keras\n",
      "Epoch 3/10\n",
      "2547/2547 [==============================] - 626s 246ms/step - loss: 0.1038 - accuracy: 0.9493 - val_loss: 0.1116 - val_accuracy: 0.9442\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.94005 to 0.94417, saving model to dlthon2.keras\n",
      "Epoch 4/10\n",
      "2547/2547 [==============================] - 626s 246ms/step - loss: 0.0898 - accuracy: 0.9571 - val_loss: 0.1083 - val_accuracy: 0.9510\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.94417 to 0.95098, saving model to dlthon2.keras\n",
      "Epoch 5/10\n",
      "2547/2547 [==============================] - 625s 246ms/step - loss: 0.0800 - accuracy: 0.9631 - val_loss: 0.0960 - val_accuracy: 0.9552\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.95098 to 0.95517, saving model to dlthon2.keras\n",
      "Epoch 6/10\n",
      "2547/2547 [==============================] - 625s 245ms/step - loss: 0.0707 - accuracy: 0.9681 - val_loss: 0.0948 - val_accuracy: 0.9561\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.95517 to 0.95611, saving model to dlthon2.keras\n",
      "Epoch 7/10\n",
      "2547/2547 [==============================] - 625s 245ms/step - loss: 0.0624 - accuracy: 0.9727 - val_loss: 0.0924 - val_accuracy: 0.9601\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.95611 to 0.96014, saving model to dlthon2.keras\n",
      "Epoch 8/10\n",
      "2547/2547 [==============================] - 625s 245ms/step - loss: 0.0584 - accuracy: 0.9749 - val_loss: 0.1066 - val_accuracy: 0.9573\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.96014\n",
      "Epoch 9/10\n",
      "2547/2547 [==============================] - 624s 245ms/step - loss: 0.0528 - accuracy: 0.9777 - val_loss: 0.0989 - val_accuracy: 0.9616\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.96014 to 0.96162, saving model to dlthon2.keras\n",
      "Epoch 10/10\n",
      "2547/2547 [==============================] - 624s 245ms/step - loss: 0.0470 - accuracy: 0.9802 - val_loss: 0.1032 - val_accuracy: 0.9602\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.96162\n"
     ]
    }
   ],
   "source": [
    "history = use_bert_model.fit([train_a_input_ids_b, train_a_attention_mask_b], train_label_ids_np, validation_split=0.2, epochs = 10, callbacks=[checkpoint], batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['상반기 첫 성과급이 예상보다 더 많이 들어왔어. 예상치 못한 기쁨이야! 옌 성과급을 예상보다 많이 받으셔서 기쁘시군요. 얼마나 많이 받으셨나요? 옌 예상보다 백만 원은 더 들어온 것 같아. 너무 행복해! 옌 성과급을 어디에 사용하고 싶으신가요? 옌 부모님께 선물을 사드릴 생각이야. 정말 기뻐하실 거야. 옌 부모님께 선물을 사드릴 생각에 기쁘시군요.',\n",
       "       '어제 갑자기 큰 아이가 자동차를 사야겠단 이야기를 하더군. 아무래도 직장을 다니면서 대중교통을 이용하는 것이 불편한가 봐. 옌 그런 일이 있으셨군요. 어떻게 하실 생각이세요? 옌 집사람과 의논도 해보았지만 답이 안 나와. 괜히 차를 몰고 다니면 또 직장동료들이나 친구들 사이에서 기사 역할을 도맡게 되는 것이 아닌가 싶기도 하고. 옌 그러시군요. 왜 그런 생각이 드시는 건가요? 옌 우리 아이가 거절을 잘 못하는 스타일이라서 그래. 괜히 기사 역할 하게 될까 봐 걱정이 많이 드는 것은 사실이야. 차를 사주어야 할지 말아야 할지 참 헷갈리는군. 옌 걱정이 많이 되시겠어요. 그런 점이 걱정되셔서 쉽게 차를 사주시지 못하시겠어요.',\n",
       "       '너 일로 와봐. 옌 네. 옌 너희 아까 손님들 앞에서 너희끼리 떠들면서 웃었니? 옌 네??.아니요 아까 손님께서 말 거셔서 그냥 웃은 거에요. 옌 여기가 어디라고 웃어 너희 일하러 왔지 놀러왔니? 옌 죄송합니다. 옌 하여튼 가봐.일이나 제대로 하면 말도 안해 옌 네 그럼 가 보겠습니다. 옌 아참가서 식빵 4개 모닝롤 3개 단호박빵 3개 가져와. 옌 매니저님 죄송한데 다시 한번만 말씀해 주실 수 있으실까요? 옌 하.됐어 내가 할라니까 옌 죄송합니다.',\n",
       "       ...,\n",
       "       '저기요. 쿠팡잇츠에서 시킨 사람인데요. 빙수 다른데요? 옌 네? 혹시 주문번호가 어떻게 되나요.? 옌 203번이요 옌 아. 사인머스켓 시켜셨지요? 문제인가요? 옌 네. 샤인머스켓에서 단맛이 없고 사과처럼 퍼석 옌 그럴리가 다른 고객님들은 아직 컴플레인 혹시 불 옆에서 하시거나 다른 음식이랑 아니신가요? 옌 팥빙수를 그냥 먹지 조리를 어떻게 미친거아니야. 그리고 내가 다른 음식이랑 왜 섞어서 먹어요. 됐고 빨리 환불해주세요. 옌 분 음식이 어떻게 할까요? 옌 또 우리만 손해니까 그냥 환불해드려. 옌 네. 알겠습니다. 고객님 들여 지금 환불해드리겠습니다.',\n",
       "       '아이들을 가르치는 일이 좋아서 시작했는데 이젠 교탁에 서 있기도 힘에 부쳐. 이젠 무리야. 옌 몸이 아파서 정리를 고민하시다니 아주 속상하시겠어요. 옌 배로 낳은 아이와 같이 사랑으로 보듬었는데 그 체력이 다한 것 같아 눈물이 나네. 옌 지금 상황에서 할 수 있는 최선의 방법이 무엇일까요? 옌 최대한 오래 일을 하고 싶지만 교장 선생님과 면담을 해서 퇴직 시기를 정해야겠어. 옌 면담을 통해 원활한 조율을 이루시길 바라요.',\n",
       "       '내 생일인데 축하 한마디 없네. 아들 키워봤자 다 소용없어. 옌 축하를 못 받아 기분이 안 좋으시군요. 옌 돈 쓰고 놀러 다닐 줄만 알지. 부모님 생각은 요만큼도 안 해. 옌 이 상황에서 어떻게 하실지 생각해 보셨나요? 옌 남편이랑 같이 맛있는 거나 먹고 와야겠어. 옌 남편분과 같이 좋은 시간 가졌으면 좋겠어요.'],\n",
       "      dtype='<U896')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_a_input_ids, test_a_attention_mask = bert_encode(test_data_a, 240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label_ids = make_laber(test_a_input_ids,246458, 240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label_ids_np = np.concatenate(test_label_ids, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_a_input_ids_b, test_a_attention_mask_b = bert_encode(test_data_b, 240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/199 [==============================] - 42s 213ms/step - loss: 0.0969 - accuracy: 0.9630\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09694472700357437, 0.9629745483398438]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_bert_model.evaluate([test_a_input_ids_b, test_a_attention_mask_b], test_label_ids_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_a_pridict = use_bert_model.predict([test_a_input_ids_b, test_a_attention_mask_b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_a_predictions = test_a_pridict.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_a_predictions[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label_ids_np[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이번 시험 점수가 떨어지지 않아서 다행이야. 시험 점수가 떨어지지 않으셨군요. 이번 시험을 위해 무슨 노력을 하셨나요? 수업 시간에 집중해서 공부했어. 내가 원하는 학과를 못 갈까 봐 열심히 했어. 노력한 만큼 좋은 결과가 나와서 행복하시겠어요.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_b[1000]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
